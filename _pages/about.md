---
layout: page
title: About
permalink: /about/
---

## PhD Research

I am a PhD student at King's College London and Imperial College London studying Computer Science in the [Safe and Trusted AI Centre for Doctoral Training](https://safeandtrustedai.org). My doctoral research is about framing "explanation" in the context of *multi-agent communication* so as to explcitly take into account the recipient of the explanation (who we call the *explainee*). This research is inherently interdisciplinary; notably involving philosophy, social science, and cognitive science. To understand the research, lets first get a little meta: in this very paragraph I am attempting to explain my PhD to a "general audience". For all I know, you the reader may be another AI researcher or artist with no knowledge of AI (but hopefully a passing interest at least!). In order to make sure the most number of people reading this page can understand it I am having to make estimations about what you may or may not know, and try to efficiently get my point across without needing a book-length explanation (you can catch that in the PhD thesis itself).

This brings us to the core question of my research: "How should an AI system construct an effective explanation for a given explainee (or set of explainees)?". However, the follow-up question is perhaps the more important one: "Why bother?". The answer to this second question lies primarily in two disturbing problems in AI. Firstly, the most recent state-of-the-art approaches to solving problems in AI are famously inscrutable. This family of approaches, called "deep learning", produce opaque, complex systems. Due to our lack of understanding regarding their internal processes these machines cannot be deployed into real-world situations in which they need to be held accountable. This ranges from critical safety situations such as medical control systems to simple regulatory requirements regarding the uses of people's data. These issues fall under the problem of "fairness, accountability, and transparency" (FAT) in AI. This demands not just explanations that satisfy engineers and researchers, but also ones that facilitate understanding for anyone who is effected by an AI system.

The second reason to study explanation is what is known as [the alignment problem](https://www.amazon.co.uk/Alignment-Problem-Machine-Learning-Values/dp/0393635821) in AI. This problem is effectively an extension of the FAT problem to situations in which the AI system itself poses a safety concern. The alignment problem asks, "what happens as AI systems become evermore capable at solving problems?", and "how might we point such systems at actually solving our problems?". Once again, there is the need to understand some aspect of the AI system, in this case its goals, and thus the need for explanations.

## Other Interests

Outside of my PhD research I am broadly interested in language and cognitive science. This has led me to studying the exciting field of _emergent communication_ in reinforcement learning, and to studying inductive biases for deep learning systems that could facilitate more robust behaviour by having systems explicitly mimick cognitive functions such as analogy-making and information retrieval. 

## Teaching

2021/22 Semester 2:

* 6CCS3ML - Machine Learning

## Publications

Dylan Cope, Peter McBurney, 2021, _A Measure of Explanatory Effectiveness_, 1st International Workshop on Trusted Automated Decision-Making

Dylan Cope, Nandi Schoots, 2020, _Learning to Communicate with Strangers via Channel Randomisation Methods_, 4th NeurIPS Workshop on Emergent Communication


<div itemscope itemtype="https://schema.org/Person"><a itemprop="sameAs" content="https://orcid.org/0000-0003-1147-8010" href="https://orcid.org/0000-0003-1147-8010" target="orcid.widget" rel="me noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon">https://orcid.org/0000-0003-1147-8010</a></div>
